{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Libs\n",
    "from notebookutils import mssparkutils\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pyspark.sql.functions import to_timestamp, col, explode, lit\n",
    "from pyspark.sql.types import StructType, DateType, StructField, StringType, IntegerType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "is_incremental = False # True or False\n",
    "start_date_manual = datetime(2025, 4, 20)\n",
    "end_date_manual = datetime(2025, 4, 24)\n",
    "landing_path = \"Files/landing/\"\n",
    "loaded_path = \"Files/loaded/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Creates the folders paths\n",
    "mssparkutils.fs.mkdirs(landing_path)\n",
    "mssparkutils.fs.mkdirs(loaded_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def extract_max_date_from_files(folder_path):\n",
    "    \"\"\"\n",
    "    Function to extract max_date from files when is_incremental is true.\n",
    "    \"\"\"\n",
    "\n",
    "    # List existing filenames from folder_path\n",
    "    files = mssparkutils.fs.ls(folder_path)\n",
    "\n",
    "    # A nullable list\n",
    "    dates = []\n",
    "\n",
    "    # From each file extracts the date and append to the list\n",
    "    for file in files:\n",
    "        filename = file.path.rsplit(\"/\", 1)[-1]\n",
    "        # Ensure json extension and format with date\n",
    "        if filename.endswith(\".json\") and len(filename) == len(\"YYYY-MM-DD.json\"):\n",
    "            date_str = filename[:-5]\n",
    "            # Try append with date format\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
    "                dates.append(date_obj)\n",
    "            except ValueError:\n",
    "                print(f\"Ignored: {filename}\")  \n",
    "\n",
    "    # Extract max_date from the list\n",
    "    max_date = max(dates) if dates else None\n",
    "\n",
    "    return max_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Define the start and end dates by mode\n",
    "if is_incremental:\n",
    "    max_date = extract_max_date_from_files(loaded_path)\n",
    "    start_date = max_date + timedelta(days=1)\n",
    "    end_date = date.today() \n",
    "else:\n",
    "    start_date = start_date_manual\n",
    "    end_date = end_date_manual\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Getting data from blob storage\n",
    "def date_range(start, end):\n",
    "    while start <= end:\n",
    "        yield start.strftime(\"%Y-%m-%d\")\n",
    "        start += timedelta(days=1)\n",
    "\n",
    "for date_str in date_range(start_date, end_date):\n",
    "    url = \"https://raw.githubusercontent.com/\" \\\n",
    "        \"alisonpezzott/incr-json-spark-ms-fabric-sample/\" \\\n",
    "        f\"refs/heads/main/snapshots/completed/{date_str}.json\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        local_file = f\"/tmp/{date_str}.json\"\n",
    "        with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "        mssparkutils.fs.cp(f\"file:{local_file}\", f\"{landing_path}{date_str}.json\")\n",
    "        print(f\"{date_str}.json saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Error downloading {url} (status {response.status_code})\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# JSON to dataframe\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"snapshot\", DateType(), True),\n",
    "    StructField(\"data\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"branch\", StringType(), True),\n",
    "            StructField(\"stocks\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"sku\", StringType(), True),\n",
    "                    StructField(\"qty\", IntegerType(), True)\n",
    "                ])\n",
    "            ), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .json(landing_path)\n",
    "\n",
    "df_exploded = df.withColumn(\"data\", explode(\"data\")) \\\n",
    "    .withColumn(\"stocks\", explode(\"data.stocks\")) \\\n",
    "    .withColumn(\"snapshot\", col(\"snapshot\")) \\\n",
    "    .withColumn(\"branch\", col(\"data.branch\")) \\\n",
    "    .withColumn(\"sku\", col(\"stocks.sku\")) \\\n",
    "    .withColumn(\"qty\", col(\"stocks.qty\")) \\\n",
    "    .select(\"snapshot\", \"branch\", \"sku\", \"qty\")\n",
    "\n",
    "display(df_exploded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Save to delta tables\n",
    "mode = 'append' if is_incremental else 'overwrite'\n",
    "df_exploded.write.mode(mode).saveAsTable('fact_stocks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Moves the files from landing_path to loaded_path\n",
    "files = mssparkutils.fs.ls(landing_path)\n",
    "\n",
    "for file in files:\n",
    "    source_path = file.path\n",
    "    file_name = file.name\n",
    "    target_path = f\"{loaded_path}{file_name}\"\n",
    "    \n",
    "    mssparkutils.fs.mv(source_path, target_path)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5
}
